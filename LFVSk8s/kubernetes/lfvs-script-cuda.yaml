apiVersion: batch/v1
kind: Job
metadata:
  # name of the job
  name: rohit-choudhary-lfvs-without-cpu13-20-cuda5

spec:
  template:
    spec:
      # List of containers belonging to the job starts here
      containers:
      # container name used for pod creation
      - name: lfvs-container
        # container image from the registry
        image: ccu.uni-konstanz.de:5000/rohit.choudhary/lfvs-without-cpu13-cuda:latest
        #command: ['sleep', '1h']

        # make sure the new image is always pulled fresh from the repository
        imagePullPolicy: Always

        # container resources requested from the node
        resources:
          # requests are minimum resourcerequirements
          requests:
            # this gives us a minimum 2 GiB of main memory to work with.
            memory: "2Gi"

          # limits are maximum resource allocations
          limits:
            # this gives an absolute limit of 3 GiB of main memory.
            # exceeding it will mean the container exits immediately with an error.
            #memory: "10Gi"

            # this requests a number of GPUs. GPUs will be allocated to the container
            # exclusively. No fractional GPUs can be requested.
            # When executing nvidia-smi in the container, it should show exactly this
            # number of GPUs.
            #
            # PLEASE DO NOT SET THE NUMBER TO ZERO, EVER, AND ALWAYS INCLUDE THIS LINE.
            # ALWAYS PUT IT IN THE SECTION "limits", NOT "requests".
            #
            # It is a known limitation of nVidias runtime that if zero GPUs are requested,
            # then actually *all* GPUs are exposed in the container.
            # We are looking for a fix to this.
            #
            nvidia.com/gpu: "1"

        # Environment variables set when running the image,
        # which can for example used to configure your application.
        env:
        - name: PYTHONUNBUFFERED
          value: "1"

        # the command which is executed after container creation
        command: ["/application/run.sh"]
        #command: ["/bin/bash"]

        # list of mount paths within the container which will be
        # bound to persistent volumes.
        volumeMounts:
        # /tmp/data is where mnist.py will download the training data, unless it is
        # already there. This is the path where we mount the persistent volume.
        # Thus, for the second run of this container, the data will
        # not be downloaded again.
        - mountPath: "/tmp/data"
          # name of the volume for this path (from the below list)
          name: pvc-mnist
        # this is the default log path where the application will write to tensorboard
        - mountPath: "/saver/"
          # name of the volume for this path (from the below list)
          name: pvc-mnist-tb

      # login credentials to the docker registry.
      # for convenience, a readonly credential is provided as a secret in each namespace.
      imagePullSecrets:
      - name: registry-ro-login

      # containers will never restart
      restartPolicy: Never

      volumes:
        # User-defined name of the persistent volume within this configuration.
        # This can be different from the name of the PVC.
        - name: pvc-mnist
          persistentVolumeClaim:
            # name of the PVC this volume binds to
            claimName: rohit-choudhary-lfvs-data
        - name: pvc-mnist-tb
          persistentVolumeClaim:
            # name of the PVC this volume binds to
            claimName: rohit-choudhary-lfvs-tensordata

  # number of retries after failure.
  # since we typically have to fix something in this case, set to zero by default.
  backoffLimit: 0

